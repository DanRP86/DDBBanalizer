#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Universal CSV/Excel Analyzer — Simple Config (EN V0.4e)
-------------------------------------------------------
Highlights:
- One-sheet CONFIG for non-technical users.
- Phase 1 (no config): creates initial_report + config_SIMPLE.
- Phase 2 (config detected): creates final_report + email drafts grouped by ANY column.
- Email drafts are saved to disk: .MSG (if Outlook available) or .HTML as a fallback. Never sends.
- Control rows in CONFIG:
  • "Emails": set active=YES and domain=<column to group by> (Country, Landlord Name, Product Line, Contact, etc.).
     - If the chosen column contains emails, it generates one email per address.
     - Otherwise, it generates one email per value of that column and collects recipients from Contact or any column with emails.
  • "Main ID": optional. Set domain=<column name> to force main ID for reporting and linking.
- Fix: pandas Index truth-value error replaced by len(index) checks.
- Fix: date parsing warnings by detecting ISO yyyy-mm-dd and dayfirst=False when applicable.

Requirements: pandas, numpy, openpyxl (optional pywin32 for .MSG)
"""

import argparse
import csv
import re
from datetime import datetime
from pathlib import Path
from typing import Optional, Tuple, Dict, List

import numpy as np
import pandas as pd

# =======================
# File picker (Tkinter optional)
# =======================
try:
    import tkinter as tk
    from tkinter import filedialog
    TK_AVAILABLE = True
except Exception:
    TK_AVAILABLE = False

DEFAULT_DIR = Path.cwd()

def pick_file_dialog() -> Optional[Path]:
    if not TK_AVAILABLE:
        return None
    root = tk.Tk(); root.withdraw()
    selected = filedialog.askopenfilename(
        initialdir=str(DEFAULT_DIR),
        title="Select the data file",
        filetypes=[("CSV/TSV", "*.csv *.tsv *.txt"), ("Excel", "*.xlsx *.xls"), ("All", "*.*")],
    )
    root.destroy()
    return Path(selected) if selected else None

# =======================
# IO helpers
# =======================
SEP_ALIASES = {"tab": "\t", "comma": ",", "semicolon": ";", "pipe": "|"}

def normalize_sep_arg(sep_arg: Optional[str]) -> Optional[str]:
    if not sep_arg: return None
    s = sep_arg.strip().lower(); return SEP_ALIASES.get(s, sep_arg)

def detect_bom_encoding(raw: bytes) -> Optional[str]:
    if raw.startswith(b"\xff\xfe\x00\x00"): return "utf-32le"
    if raw.startswith(b"\x00\x00\xfe\xff"): return "utf-32be"
    if raw.startswith(b"\xff\xfe"): return "utf-16le"
    if raw.startswith(b"\xfe\xff"): return "utf-16be"
    if raw.startswith(b"\xef\xbb\xbf"): return "utf-8-sig"
    return None

def try_chardet(raw: bytes) -> Optional[str]:
    try:
        import chardet  # type: ignore
        guess = chardet.detect(raw)
        return guess.get("encoding")
    except Exception:
        return None

def detect_separator(text: str, max_lines: int = 50) -> Optional[str]:
    lines = [ln for ln in text.splitlines() if ln.strip()][:max_lines]
    if not lines: return None
    sample = "\n".join(lines)
    try:
        dialect = csv.Sniffer().sniff(sample, delimiters=[",", ";", "\t", "|"])
        return dialect.delimiter
    except Exception:
        pass
    candidates = [",", ";", "\t", "|"]
    best_sep, best_score = None, -1e9
    for sep in candidates:
        counts = [ln.count(sep) for ln in lines]
        mean_c = float(np.mean(counts)); var_c = float(np.var(counts))
        score = mean_c - var_c
        if mean_c >= 1 and score > best_score:
            best_sep, best_score = sep, score
    return best_sep

def smart_read_csv_with_guess(path: Path, forced_encoding: Optional[str] = None, forced_sep: Optional[str] = None) -> Tuple[pd.DataFrame, dict]:
    if forced_encoding or forced_sep:
        enc = forced_encoding or "utf-8"
        sep = normalize_sep_arg(forced_sep) or None
        if sep is None:
            with open(path, "rb") as f: raw = f.read(200_000)
            text = raw.decode(enc, errors="replace"); sep = detect_separator(text) or ","
        df = pd.read_csv(path, sep=sep, encoding=enc, engine="python")
        return df, {"encoding": enc, "sep": sep, "forced": True}
    with open(path, "rb") as f: raw = f.read(200_000)
    encodings_to_try = []
    bom = detect_bom_encoding(raw)
    if bom: encodings_to_try.append(bom)
    encodings_to_try += ["utf-8", "utf-8-sig", "latin1", "cp1252", "utf-16", "utf-16le", "utf-16be", "utf-32"]
    ch = try_chardet(raw)
    if ch and ch not in encodings_to_try: encodings_to_try.append(ch)
    last_err = None
    for enc in encodings_to_try:
        try:
            text = raw.decode(enc, errors="replace")
        except Exception as e:
            last_err = e; continue
        sep = detect_separator(text) or None
        try:
            if sep:
                df = pd.read_csv(path, sep=sep, encoding=enc, engine="python"); return df, {"encoding": enc, "sep": sep, "forced": False}
            else:
                df = pd.read_csv(path, sep=None, encoding=enc, engine="python"); return df, {"encoding": enc, "sep": "auto", "forced": False}
        except Exception as e:
            last_err = e
        for sep_try in [",", ";", "\t", "|"]:
            try:
                df = pd.read_csv(path, sep=sep_try, encoding=enc, engine="python"); return df, {"encoding": enc, "sep": sep_try, "forced": False}
            except Exception as e2:
                last_err = e2
    raise ValueError(f"Could not read CSV. Last error: {last_err}")

def select_excel_sheet_with_most_data(xlsx_path: Path, max_rows_sample: int = 5000) -> str:
    xl = pd.ExcelFile(xlsx_path)
    best, score = None, -1
    for sh in xl.sheet_names:
        try:
            df_tmp = xl.parse(sh, nrows=max_rows_sample)
            sc = int(df_tmp.notna().sum().sum())
            if sc > score: score, best = sc, sh
        except Exception:
            pass
    return best or xl.sheet_names[0]

def smart_read_any(path: Path, forced_encoding: Optional[str] = None, forced_sep: Optional[str] = None, input_sheet: Optional[str] = None) -> Tuple[pd.DataFrame, dict]:
    if path.suffix.lower() in {'.xlsx', '.xls'}:
        sheet = input_sheet or select_excel_sheet_with_most_data(path)
        df = pd.read_excel(path, sheet_name=sheet)
        return df, {"encoding": "binary", "sep": f"excel:{sheet}", "sheet": sheet, "forced": False}
    return smart_read_csv_with_guess(path, forced_encoding, forced_sep)

# =======================
# Inference & checks
# =======================
EMAIL_RE = re.compile(r'^[A-Za-z0-9._%+\-]+@[A-Za-z0-9.\-]+\.[A-Za-z]{2,}$')
ALNUM_RE = re.compile(r'^[A-Za-z0-9\-_]+$')

def infer_kind(s: pd.Series, sample_size: int = 500) -> str:
    s_non = s.dropna()
    if len(s_non) == 0: return "empty"
    sample = s_non.sample(min(sample_size, len(s_non)), random_state=42).astype(str).str.strip()
    if sample.str.fullmatch(EMAIL_RE).mean() > 0.6: return "email"
    if sample.str.startswith(("http://", "https://")).mean() > 0.6: return "url"
    if pd.to_datetime(sample, errors="coerce", dayfirst=True).notna().mean() > 0.7: return "date"
    pnum = pd.to_numeric(sample.str.replace('%','', regex=False), errors='coerce')
    if pnum.notna().mean() > 0.7:
        inside_0_1 = ((pnum>=0)&(pnum<=1)).mean(); inside_0_100=((pnum>=0)&(pnum<=100)).mean()
        return "percent" if (sample.str.contains('%').mean()>0.05 or max(inside_0_1,inside_0_100)>0.85) else "numeric"
    nunique = s_non.nunique(dropna=True); uniq_ratio = nunique/len(s_non)
    avg_len = sample.str.len().mean()
    if uniq_ratio>0.9 and avg_len<=40 and sample.str.fullmatch(ALNUM_RE).mean()>0.7: return "id"
    if uniq_ratio<0.2 and avg_len<30: return "categorical"
    return "free_text"

CRIT_TO_NUM = {"LOW":1, "MEDIUM":2, "HIGH":3, "VERY_HIGH":4}
ISSUE_DEFAULT = {
    "missing":"MEDIUM",
    "numeric_parse_fail":"HIGH",
    "outlier_iqr":"MEDIUM",
    "outlier_magnitude":"HIGH",
    "percent_parse_fail":"HIGH",
    "percent_out_of_range":"HIGH",
    "percent_outlier_iqr":"MEDIUM",
    "date_parse_fail":"HIGH",
    "date_out_of_bounds":"VERY_HIGH",
    "email_format":"HIGH",
    "url_format":"MEDIUM",
    "domain_violation":"HIGH",
    "regex_violation":"HIGH",
    "text_length_outlier":"LOW",
    "weird_symbols_ratio":"MEDIUM",
}

def crit_num(issue_code: str) -> int: return CRIT_TO_NUM[ISSUE_DEFAULT.get(issue_code,"LOW")]

def parse_bool(x, default=False) -> bool:
    if x is None or (isinstance(x,float) and pd.isna(x)): return default
    if isinstance(x,bool): return x
    s=str(x).strip().lower(); return s in {"true","1","yes","y","si","sí","on"}

def to_priority_num(s: str) -> int:
    if not isinstance(s,str): return 0
    return CRIT_TO_NUM.get(s.strip().upper(),0)

# =======================
# Severity builder (simple)
# =======================

def iqr_outliers(x: pd.Series, k: float = 1.5) -> pd.Series:
    x = pd.to_numeric(x, errors="coerce")
    q1 = x.quantile(0.25); q3 = x.quantile(0.75); iqr = q3-q1
    if pd.isna(iqr) or iqr==0: return pd.Series(False, index=x.index)
    return (x < q1-k*iqr) | (x > q3+k*iqr)

def magnitude_outliers(x: pd.Series, factor: float = 1e3) -> pd.Series:
    x = pd.to_numeric(x, errors="coerce"); med = x.median()
    if pd.isna(med) or med==0: return pd.Series(False, index=x.index)
    return x > abs(med)*factor

def apply_base_priority(col_sev_arr: np.ndarray, mask: pd.Series, issue_code: str, base_priority_num: int) -> np.ndarray:
    default_num = crit_num(issue_code); num = max(default_num, base_priority_num or 0)
    if num<=0: return col_sev_arr
    arr_mask = np.where(mask.to_numpy(), num, 0)
    return np.maximum(col_sev_arr, arr_mask)

# Parse date values from config avoiding warning with ISO dates
ISO_DATE_RE = re.compile(r"^\d{4}-\d{2}-\d{2}$")

def parse_cfg_date(val):
    if val in (None, ""): return None
    s = str(val).strip()
    if ISO_DATE_RE.match(s):
        try:
            return pd.to_datetime(s, format="%Y-%m-%d", errors="coerce")
        except Exception:
            return pd.to_datetime(s, errors="coerce", dayfirst=False)
    return pd.to_datetime(s, errors="coerce", dayfirst=True)

def build_severity_simple(df: pd.DataFrame, config_rows: pd.DataFrame, coverage_target: float,
                          future_days: int, main_id_col: Optional[str]) -> Tuple[pd.DataFrame, Dict[str, dict], dict]:
    sev = pd.DataFrame(0, index=df.index, columns=df.columns, dtype=np.int8)
    cfg = config_rows.copy(); cfg.columns=[str(c).strip().lower() for c in cfg.columns]
    control = {}
    forced_main_id = None
    if not cfg.empty and 'columna' in cfg.columns:
        # Emails row (control)
        mask_emails = cfg['columna'].astype(str).str.strip().str.lower().eq('emails')
        if mask_emails.any():
            last = cfg[mask_emails].iloc[-1]
            control = {
                'email_to': str(last.get('min_propuesto','') or '').strip(),
                'group_col': str(last.get('dominio','') or '').strip(),
                'generate': parse_bool(last.get('activo','NO'), default=False),
            }
            cfg = cfg[~mask_emails]
        else:
            # Backward-compat: __CONTROL__
            mask_ctrl = cfg['columna'].astype(str).str.strip().eq('__CONTROL__')
            if mask_ctrl.any():
                last = cfg[mask_ctrl].iloc[-1]
                control = {
                    'email_to': str(last.get('min_propuesto','') or '').strip(),
                    'group_col': str(last.get('dominio','') or '').strip(),
                    'generate': parse_bool(last.get('activo','NO'), default=False),
                }
                cfg = cfg[~mask_ctrl]
        # Main ID row
        mask_mainid = cfg['columna'].astype(str).str.strip().str.lower().eq('main id')
        if mask_mainid.any():
            row = cfg[mask_mainid].iloc[-1]
            forced_main_id = str(row.get('dominio','') or '').strip()
            cfg = cfg[~mask_mainid]

    rule_map: Dict[str,dict] = {}
    for _, r in cfg.iterrows():
        col = str(r.get('columna','') or '').strip()
        if not col: continue
        rule_map[col] = {
            'tipo': str(r.get('tipo','') or '').strip().lower(),
            'min': r.get('min_propuesto', None),
            'max': r.get('max_propuesto', None),
            'dominio': r.get('dominio', None),
            'regex': r.get('regex', None),
            'prioridad': to_priority_num(r.get('prioridad','')),
            'activo': parse_bool(r.get('activo','SI'), default=True),
            'resaltar_null': parse_bool(r.get('resaltar_null','SI'), default=True),
        }
    for col in df.columns:
        s = df[col]; rule = rule_map.get(col)
        tipo = infer_kind(s) if not rule or not rule.get('tipo') else rule['tipo']
        activo = True if not rule else bool(rule['activo'])
        base_pri = 0 if not rule else int(rule['prioridad'])
        resaltar_null = True if not rule else bool(rule['resaltar_null'])
        col_sev = np.zeros(len(s), dtype=np.int8)
        if resaltar_null:
            is_null = s.isna()
            if is_null.any(): col_sev = np.maximum(col_sev, np.where(is_null, crit_num('missing'), 0))
        if not activo:
            sev[col]=col_sev; continue
        dom_vals=set()
        if rule and rule.get('dominio') and isinstance(rule.get('dominio'),str):
            dom_vals = {v.strip() for v in str(rule.get('dominio')).split(',') if v.strip()}
        regex_pat=None
        if rule and rule.get('regex') and isinstance(rule.get('regex'),str) and rule.get('regex').strip():
            try:
                regex_pat=re.compile(str(rule.get('regex')).strip())
            except re.error:
                regex_pat=None
        min_v = rule.get('min') if rule else None
        max_v = rule.get('max') if rule else None
        try:
            if tipo=='numeric':
                x = pd.to_numeric(s, errors='coerce')
                parse_fail = s.notna() & x.isna()
                if parse_fail.any(): col_sev = apply_base_priority(col_sev, parse_fail, 'numeric_parse_fail', base_pri)
                if min_v not in (None,''):
                    m = x < float(min_v); col_sev = apply_base_priority(col_sev, m, 'percent_out_of_range', base_pri)
                if max_v not in (None,''):
                    m = x > float(max_v); col_sev = apply_base_priority(col_sev, m, 'percent_out_of_range', base_pri)
                m_iqr = iqr_outliers(x, k=1.5)
                if m_iqr.any(): col_sev = apply_base_priority(col_sev, m_iqr, 'outlier_iqr', base_pri)
                m_mag = magnitude_outliers(x, factor=1e3)
                if m_mag.any(): col_sev = apply_base_priority(col_sev, m_mag, 'outlier_magnitude', base_pri)
                if dom_vals:
                    bad = s.notna() & ~s.astype(str).isin(dom_vals)
                    if bad.any(): col_sev = apply_base_priority(col_sev, bad, 'domain_violation', base_pri)
                if regex_pat:
                    bad = s.notna() & ~s.astype(str).apply(lambda v: bool(regex_pat.fullmatch(str(v))))
                    if bad.any(): col_sev = apply_base_priority(col_sev, bad, 'regex_violation', base_pri)
            elif tipo=='percent':
                ss = s.astype(str).str.replace('%','', regex=False)
                x = pd.to_numeric(ss, errors='coerce')
                parse_fail = s.notna() & x.isna()
                if parse_fail.any(): col_sev = apply_base_priority(col_sev, parse_fail, 'percent_parse_fail', base_pri)
                lo = 0.0 if (min_v in (None,'')) else float(min_v)
                hi = 100.0 if (max_v in (None,'')) else float(max_v)
                bad = (x < lo) | (x > hi)
                if bad.any(): col_sev = apply_base_priority(col_sev, bad, 'percent_out_of_range', base_pri)
                m_iqr = iqr_outliers(x, k=1.5)
                if m_iqr.any(): col_sev = apply_base_priority(col_sev, m_iqr, 'percent_outlier_iqr', base_pri)
            elif tipo=='date':
                x = pd.to_datetime(s, errors='coerce', dayfirst=True)
                parse_fail = s.notna() & x.isna()
                if parse_fail.any(): col_sev = apply_base_priority(col_sev, parse_fail, 'date_parse_fail', base_pri)
                if x.notna().any():
                    lower = parse_cfg_date(min_v) if min_v not in (None,'') else pd.Timestamp('1900-01-01')
                    upper = parse_cfg_date(max_v) if max_v not in (None,'') else (pd.Timestamp.today()+pd.Timedelta(days=future_days))
                    bad = ((x < lower) | (x > upper)) & x.notna()
                    if bad.any(): col_sev = apply_base_priority(col_sev, bad, 'date_out_of_bounds', base_pri)
            elif tipo=='email':
                bad = s.notna() & ~s.astype(str).str.fullmatch(EMAIL_RE)
                if bad.any(): col_sev = apply_base_priority(col_sev, bad, 'email_format', base_pri)
            elif tipo=='url':
                bad = s.notna() & ~s.astype(str).str.startswith(("http://","https://"))
                if bad.any(): col_sev = apply_base_priority(col_sev, bad, 'url_format', base_pri)
            elif tipo=='categorical':
                if dom_vals:
                    bad = s.notna() & ~s.astype(str).isin(dom_vals)
                    if bad.any(): col_sev = apply_base_priority(col_sev, bad, 'domain_violation', base_pri)
                if regex_pat:
                    bad = s.notna() & ~s.astype(str).apply(lambda v: bool(regex_pat.fullmatch(str(v))))
                    if bad.any(): col_sev = apply_base_priority(col_sev, bad, 'regex_violation', base_pri)
            elif tipo=='free_text':
                s_str = s.dropna().astype(str)
                if len(s_str)>0:
                    lens = s.astype(str).str.len(); m_len = iqr_outliers(lens, k=2.0)
                    if m_len.any(): col_sev = apply_base_priority(col_sev, m_len, 'text_length_outlier', base_pri)
                    sym_ratio = s.astype(str).apply(lambda x: sum(ch in "!@#$%^&*()=+[]{}<>~`" for ch in x)/max(1,len(x)))
                    arr = sym_ratio.to_numpy(dtype=float); med=float(np.median(arr)); mad=float(np.median(np.abs(arr-med)))
                    thr = med + 5*mad if mad>0 else 0.5
                    m_sym = sym_ratio > thr
                    if m_sym.any(): col_sev = apply_base_priority(col_sev, m_sym, 'weird_symbols_ratio', base_pri)
        except Exception:
            pass
        sev[col]=col_sev.astype(np.int8)
    if forced_main_id: control['main_id_forced'] = forced_main_id
    return sev, rule_map, control

# =======================
# Main ID detection
# =======================
ID_NAME_HINTS = [r"\b(id|code|codigo|código|employee|empleado|vin|matricula|matrícula|plate|nif|dni|cif|cedula|cédula|passport)\b"]

def score_id_candidate(col_name: str, s: pd.Series) -> float:
    s_non = s.dropna(); total=len(s)
    if total==0: return 0.0
    nunique = s_non.nunique(dropna=True)
    uniq_ratio = (nunique/len(s_non)) if len(s_non) else 0.0
    s_str = s_non.astype(str).str.strip()
    avg_len = float(s_str.str.len().mean()) if len(s_str) else 0.0
    alnum_ratio = float(s_str.str.fullmatch(ALNUM_RE).mean()) if len(s_str) else 0.0
    as_date = pd.to_datetime(s_non, errors='coerce', dayfirst=True).notna().mean() if len(s_non) else 0.0
    as_num  = pd.to_numeric(s_non, errors='coerce').notna().mean() if len(s_non) else 0.0
    name_hint = any(re.search(pat, col_name, flags=re.IGNORECASE) for pat in ID_NAME_HINTS)
    score=0.0
    if name_hint: score+=3.0
    if uniq_ratio>=0.98: score+=3.0
    elif uniq_ratio>=0.9: score+=2.0
    if 6<=avg_len<=40: score+=1.0
    if alnum_ratio>=0.8: score+=1.0
    if as_date<=0.05: score+=0.5
    if as_num<=0.5: score+=0.5
    return score

def detect_main_id(df: pd.DataFrame) -> str:
    if df.shape[1]==0: return 'row_id'
    best, best_score = df.columns[0], -1.0
    for c in df.columns:
        sc = score_id_candidate(c, df[c])
        if sc>best_score: best, best_score = c, sc
    return best

# =======================
# Simple CONFIG generator & reader
# =======================
SIMPLE_CONFIG_COLUMNS = [
    'columna','tipo','min_propuesto','max_propuesto','dominio','regex','prioridad','activo','resaltar_null'
]

def generate_simple_config(input_path: Path, df: pd.DataFrame, out_path: Path,
                           coverage_target: float, future_days: int, used_sheet: Optional[str]) -> None:
    rows=[]
    detected_main_id = detect_main_id(df)
    for col in df.columns:
        kind=infer_kind(df[col]); min_p=''; max_p=''; domain_vals=''; regex=''
        if kind in ('numeric','percent'):
            x = pd.to_numeric(df[col].astype(str).str.replace('%','',regex=False), errors='coerce')
            if x.notna().any(): min_p=float(x.quantile(0.01)); max_p=float(x.quantile(0.99))
        elif kind=='date':
            x = pd.to_datetime(df[col], errors='coerce', dayfirst=True)
            if x.notna().any(): min_p=x.quantile(0.01).date().isoformat(); max_p=x.quantile(0.99).date().isoformat()
        elif kind=='categorical':
            vc=df[col].value_counts(dropna=True)
            if len(vc)>0: domain_vals=",".join(map(str, vc.head(15).index.tolist()))
        rows.append({'columna':col,'tipo':kind,'min_propuesto':min_p,'max_propuesto':max_p,
                     'dominio':domain_vals,'regex':regex,'prioridad':'MEDIUM','activo':'SI','resaltar_null':'SI'})
    # Meta rows: Emails and Main ID
    rows.append({'columna':'Emails','tipo':'meta','min_propuesto':'','max_propuesto':'',
                 'dominio':'Country','regex':'','prioridad':'','activo':'NO','resaltar_null':''})
    rows.append({'columna':'Main ID','tipo':'meta','min_propuesto':'','max_propuesto':'',
                 'dominio':detected_main_id,'regex':'','prioridad':'','activo':'','resaltar_null':''})

    cfg=pd.DataFrame(rows, columns=SIMPLE_CONFIG_COLUMNS)
    with pd.ExcelWriter(out_path, engine='openpyxl') as w:
        cfg.to_excel(w, index=False, sheet_name='CONFIG')
        meta_df=pd.DataFrame({'key':['input_file','input_sheet','coverage_target','future_days','generated_on','rows','cols'],
                              'value':[str(input_path.name),(used_sheet or ''),coverage_target,future_days,datetime.now().isoformat(),len(df),df.shape[1]]})
        meta_df.to_excel(w, index=False, sheet_name='META')

def read_simple_config(cfg_path: Path) -> Tuple[pd.DataFrame, dict]:
    xl = pd.ExcelFile(cfg_path)
    cfg = xl.parse('CONFIG'); cfg.columns=[str(c).strip().lower() for c in cfg.columns]
    for c in SIMPLE_CONFIG_COLUMNS:
        if c not in cfg.columns: cfg[c]=''
    return cfg[SIMPLE_CONFIG_COLUMNS].copy(), {'has_meta': 'META' in xl.sheet_names}

# =======================
# Export report
# =======================
from openpyxl.styles import PatternFill, Font

def export_excel(df_raw: pd.DataFrame, sev_df: pd.DataFrame, out_path: Path,
                 summary_extra: Dict[str,str], main_id_name: str) -> None:
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with pd.ExcelWriter(out_path, engine='openpyxl') as w:
        summary_rows={'metric':[], 'value':[]}
        for k,v in { 'rows': len(df_raw), 'columns': df_raw.shape[1], **summary_extra }.items():
            summary_rows['metric'].append(k); summary_rows['value'].append(v)
        pd.DataFrame(summary_rows).to_excel(w, index=False, sheet_name='Summary')
        df_raw.to_excel(w, index=False, sheet_name='Dataset')
        pd.DataFrame({'key':['main_id'], 'value':[main_id_name]}).to_excel(w, index=False, sheet_name='Main_ID')
        ws=w.sheets['Dataset']
        FILLS={1:PatternFill(fill_type='solid', fgColor='FFF2CC'), 2:PatternFill(fill_type='solid', fgColor='FCE4D6'),
               3:PatternFill(fill_type='solid', fgColor='F8CBAD'), 4:PatternFill(fill_type='solid', fgColor='C00000')}
        WHITE=Font(color='FFFFFF')
        n_rows, n_cols = df_raw.shape
        for r in range(n_rows):
            for c in range(n_cols):
                sev=int(sev_df.iat[r,c])
                if sev>0:
                    cell = ws.cell(row=r+2, column=c+1)
                    cell.fill = FILLS.get(sev)
                    if sev==4: cell.font=WHITE

# =======================
# Email generation (MSG/HTML)
# =======================

def extract_emails(text: str) -> List[str]:
    if not isinstance(text,str) or not text: return []
    pat=re.compile(r"[A-Za-z0-9._%+\-]+@[A-Za-z0-9.\-]+\.[A-Za-z]{2,}")
    return pat.findall(text)

def html_pill(label: str, bg="#eef2ff", fg="#1f2a6b") -> str:
    return f'<span style="display:inline-block;padding:2px 6px;border-radius:10px;background:{bg};color:{fg};font-size:12px;">{label}</span>'

def html_escape(s: str) -> str:
    return (str(s).replace('&','&amp;').replace('<','&lt;').replace('>','&gt;').replace('"','&quot;').replace("'","&#39;"))

def build_row_issue_summary(df: pd.DataFrame, sev_df: pd.DataFrame, id_col: str) -> pd.DataFrame:
    id_series = df[id_col].astype(str) if id_col in df.columns else pd.Series([str(i) for i in df.index], index=df.index, name='row_id')
    is_null_df = df.isna(); missing_cols_list=[]
    for i in df.index:
        miss_cols = df.columns[is_null_df.loc[i]].tolist(); missing_cols_list.append(', '.join(map(str, miss_cols)))
    sev_num = sev_df.to_numpy(); low=(sev_num==1).sum(axis=1); med=(sev_num==2).sum(axis=1); high=(sev_num==3).sum(axis=1); vhi=(sev_num==4).sum(axis=1)
    return pd.DataFrame({ id_series.name: id_series.values,
                          'MissingFields': missing_cols_list,
                          'Count_LOW': low, 'Count_MED': med, 'Count_HIGH': high, 'Count_VHIGH': vhi }, index=df.index)

def generate_emails(df: pd.DataFrame, sev_df: pd.DataFrame, control: dict, main_id: str,
                    out_dir: Path, default_contact_col: str = 'Contact') -> None:
    if not control or not control.get('generate', False): return
    group_col_name = (control.get('group_col','') or '').strip()
    if not group_col_name:
        print("[Emails] 'domain' (group-by column) is empty. Skipping email generation."); return
    # Resolve group-by column (case-insensitive)
    col_map = {str(c).strip(): c for c in df.columns}
    col_lower_map = {str(c).strip().lower(): c for c in df.columns}
    resolved_group_col = col_map.get(group_col_name) or col_lower_map.get(group_col_name.lower())
    if not resolved_group_col:
        print(f"[Emails] Group-by column '{group_col_name}' does not exist. Skipping."); return

    series_grp = df[resolved_group_col].astype(str).fillna('')
    email_like_ratio = series_grp.apply(lambda x: len(extract_emails(x))>0).mean() if len(series_grp) else 0.0

    # Build groups
    if email_like_ratio >= 0.2:
        # One group per email found in the group column
        email_to_indices: Dict[str, List[int]] = {}
        for i, cell in df[resolved_group_col].fillna('').astype(str).items():
            for em in (e.lower() for e in extract_emails(cell)):
                email_to_indices.setdefault(em, []).append(i)
        grouping = email_to_indices
        contact_col = resolved_group_col
    else:
        # One group per value in the group column
        groups = df[resolved_group_col].fillna('(no value)')
        grouping = {g: df.index[groups==g] for g in groups.unique()}
        # Determine a recipients column
        contact_col = default_contact_col if default_contact_col in df.columns else None
        if not contact_col:
            for c in df.columns:
                ser = df[c].astype(str).fillna('')
                if ser.apply(lambda x: len(extract_emails(x))>0).mean() >= 0.1:
                    contact_col = c; print(f"[Emails] Using column '{contact_col}' for recipients."); break
        if not contact_col:
            print("[Emails] No column with emails found for recipients. Drafts will be saved without 'To:'.")

    row_issues = build_row_issue_summary(df, sev_df, id_col=main_id)

    # Outlook .MSG if available, else .HTML
    outlook=None
    try:
        import win32com.client  # type: ignore
        outlook = win32com.client.Dispatch("Outlook.Application")
    except Exception:
        outlook=None; print("[Emails] Outlook/pywin32 not available. Will save .html instead of .msg.")

    out_dir.mkdir(parents=True, exist_ok=True)
    extra_to = control.get('email_to','').strip()
    generated=0

    for key, idxs in grouping.items():
        if len(idxs) == 0:
            continue
        sub = df.loc[idxs]
        sub_iss = row_issues.loc[idxs]
        mask_has_missing = sub_iss['MissingFields'].astype(str).str.len()>0
        mask_has_sev = (sub_iss[['Count_LOW','Count_MED','Count_HIGH','Count_VHIGH']].sum(axis=1) > 0)
        show = sub_iss[mask_has_missing | mask_has_sev]
        if show.empty: continue

        # Recipients
        to_list=[]
        if extra_to: to_list.extend(extract_emails(extra_to))
        if contact_col and contact_col in sub.columns:
            for val in sub[contact_col].dropna().astype(str): to_list.extend(extract_emails(val))
        if email_like_ratio >= 0.2:
            if isinstance(key,str) and key: to_list.append(key)
        to_list = sorted(set([e for e in to_list if e]))

        # HTML body (compact with pills)
        rows_html=[]
        for _, r in show.iterrows():
            id_val = html_escape(r[main_id]) if main_id in show.columns else html_escape(r.name)
            miss = html_escape(r['MissingFields']) if r['MissingFields'] else '<em>—</em>'
            pills=[]
            if r['Count_VHIGH']>0: pills.append(html_pill(f"VHIGH {int(r['Count_VHIGH'])}", "#ffe3e6", "#86181d"))
            if r['Count_HIGH']>0: pills.append(html_pill(f"HIGH {int(r['Count_HIGH'])}", "#f8d7da", "#721c24"))
            if r['Count_MED']>0: pills.append(html_pill(f"MED {int(r['Count_MED'])}", "#fdecc8", "#8a4b00"))
            if r['Count_LOW']>0: pills.append(html_pill(f"LOW {int(r['Count_LOW'])}", "#eef2ff", "#1f2a6b"))
            rows_html.append(f"""
            <tr style=\"border-bottom:1px solid #eaeaea;\">
              <td style=\"padding:6px 8px; white-space:nowrap;\">{id_val}</td>
              <td style=\"padding:6px 8px;\">{miss}</td>
              <td style=\"padding:6px 8px;\">{' '.join(pills) if pills else html_pill('—')}</td>
            </tr>
            """)
        label = key if isinstance(key,str) else str(key)
        subject = f"[Data QA] Items to review — Group: {label}"
        body_html = f"""
        <html><body style="font-family:Segoe UI, Arial, sans-serif; font-size:13px;">
        <p>Hello,</p>
        <p>Please find the list of rows with missing fields or issues detected for <b>{html_escape(label)}</b>.</p>
        <table style="border-collapse:collapse; width:100%; margin-top:8px;">
          <thead>
            <tr style="background:#f0f3f6; border-bottom:1px solid #d0d7de;">
              <th style="text-align:left;padding:6px 8px;">{html_escape(main_id)}</th>
              <th style="text-align:left;padding:6px 8px;">Missing Fields</th>
              <th style="text-align:left;padding:6px 8px;">Severities</th>
            </tr>
          </thead>
          <tbody>{''.join(rows_html)}</tbody>
        </table>
        <p style="margin-top:12px;">Thanks,<br/>Daniel</p>
        </body></html>
        """
        if outlook is not None:
            try:
                mail = outlook.CreateItem(0)
                mail.Subject = subject
                if to_list: mail.To = '; '.join(to_list)
                mail.HTMLBody = body_html
                safe = re.sub(r"[^A-Za-z0-9._\- ]+", "_", label)[:100]
                msg_path = out_dir / f"QA_group_{safe}.msg"
                mail.SaveAs(str(msg_path), 3)
                print(f"Saved MSG -> {msg_path.name} To={mail.To or '(none)'}")
                generated += 1
                continue
            except Exception as e:
                print(f"Could not save .msg, falling back to .html. Details: {e}")
        # HTML fallback
        safe = re.sub(r"[^A-Za-z0-9._\- ]+", "_", label)[:100]
        html_path = out_dir / f"QA_group_{safe}.html"
        with open(html_path, 'w', encoding='utf-8') as f: f.write(body_html)
        print(f"Saved HTML -> {html_path.name} Suggested To={', '.join(to_list) if to_list else '-'}")
        generated += 1
    print(f"Email drafts generated: {generated}")

# =======================
# MAIN
# =======================

def main():
    ap = argparse.ArgumentParser(description='Universal CSV/Excel Analyzer (EN V0.4e) — simple config (no sending)')
    ap.add_argument('-i','--input', help='Path to CSV/TSV/Excel (if omitted, opens a file picker)')
    ap.add_argument('-o','--out', help='Output Excel path (default initial/final next to input)')
    ap.add_argument('--config', help='Path to config_[input].xlsx (if omitted, autodetects)')
    ap.add_argument('--encoding', help='Force encoding (utf-16, utf-8, latin1, ...)')
    ap.add_argument('--sep', help=r'Force separator: tab, comma, semicolon, pipe or the symbol (\t, ;, , , |)')
    ap.add_argument('--input-sheet', help='Excel sheet to use (override)')
    ap.add_argument('--coverage', type=float, default=0.95, help='Coverage for categorical coherence (default 0.95)')
    ap.add_argument('--future-days', type=int, default=730, help='Allowed future window for dates (days, default 730)')
    ap.add_argument('--id-col', default='', help='Main ID column (if known)')
    args = ap.parse_args()

    # Input
    if args.input:
        input_path = Path(args.input)
        if not input_path.exists():
            print(f"File does not exist: {input_path}")
            return
    else:
        input_path = pick_file_dialog()
        if not input_path:
            print('Operation cancelled.'); return

    # Autodetect config
    if not args.config:
        auto_cfg = input_path.with_name(f"config_{input_path.stem}.xlsx")
        if auto_cfg.exists():
            print(f"Config detected: {auto_cfg.name} — FINAL phase")
            args.config = str(auto_cfg)

    # Read data
    df, meta = smart_read_any(input_path, forced_encoding=args.encoding, forced_sep=args.sep, input_sheet=args.input_sheet)
    used_sheet = meta.get('sheet') if isinstance(meta.get('sep',''),str) and str(meta.get('sep','')).startswith('excel:') else None
    print(f"Read: {input_path.name} sep={meta.get('sep')} shape={df.shape}")

    main_id = args.id_col.strip() or ''

    if args.config:
        cfg_path = Path(args.config)
        cfg_df, _ = read_simple_config(cfg_path)
        sev_df, _, control = build_severity_simple(df, cfg_df, coverage_target=args.coverage, future_days=args.future_days, main_id_col=main_id)
        # Force Main ID if present in control
        if not main_id:
            forced = control.get('main_id_forced','')
            if forced and forced in df.columns:
                main_id = forced
        if not main_id:
            main_id = detect_main_id(df)
        ts = datetime.now().strftime('%Y%m%d_%H%M')
        out_xlsx = Path(args.out) if args.out else input_path.with_name(f"final_report_{input_path.stem}_{ts}.xlsx")
        extra={'phase':'FINAL','input_sheet': used_sheet or '', 'main_id': main_id, 'config_file': Path(args.config).name}
        export_excel(df, sev_df, out_xlsx, extra, main_id)
        print(f"Final report -> {out_xlsx}")
        # Emails
        generate_emails(df, sev_df, control=control, main_id=main_id, out_dir=out_xlsx.parent)
        return

    # Phase 1: initial report + config
    if not main_id:
        main_id = detect_main_id(df)
    tmp_cfg = pd.DataFrame([{ 'columna': c, 'tipo': infer_kind(df[c]), 'min_propuesto':'', 'max_propuesto':'', 'dominio':'', 'regex':'', 'prioridad':'MEDIUM', 'activo':'SI', 'resaltar_null':'SI' } for c in df.columns])
    sev_df, _, _ = build_severity_simple(df, tmp_cfg, coverage_target=args.coverage, future_days=args.future_days, main_id_col=main_id)
    ts = datetime.now().strftime('%Y%m%d_%H%M')
    out_xlsx = Path(args.out) if args.out else input_path.with_name(f"initial_report_{input_path.stem}_{ts}.xlsx")
    extra={'phase':'INITIAL','input_sheet': used_sheet or '', 'main_id': main_id}
    export_excel(df, sev_df, out_xlsx, extra, main_id)
    print(f"Initial report -> {out_xlsx}")
    cfg_out = input_path.with_name(f"config_{input_path.stem}.xlsx")
    generate_simple_config(input_path, df, cfg_out, coverage_target=args.coverage, future_days=args.future_days, used_sheet=used_sheet)
    print("Simple config -> {}".format(cfg_out))
    print("Edit sheet 'CONFIG':\n  - Row 'Emails': active=YES, domain=<column to group by (e.g., Country, Landlord Name, Product Line, Contact)>\n  - Row 'Main ID': domain=<main ID column (optional)>\nThen run the same command again to produce FINAL + email drafts (.msg/.html).")

if __name__ == '__main__':
    main()
