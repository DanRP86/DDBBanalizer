#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Universal CSV/Excel Analyzer — Simple Config (EN V0.5.4)
-------------------------------------------------------
Changes vs 0.5.3 (user-friendly):
- Column priority now overrides per-issue severities in FINAL phase. If a user marks a column as LOW/MEDIUM/HIGH/VERY_HIGH,
  all issues in that column (including MISSING) take that severity in FINAL. INITIAL keeps previous behaviour (issue-based severities).
- Emails: "Missing Fields" column is omitted from the table if there is no missing info for the shown rows.
  Also, rows without missing no longer display "—"; the cell is blank instead.
- Minor: kept CONFIG structure but behaviour is simplified by the new column-priority override in FINAL.
- Includes requirements.txt and a friendly quick-start guide (separate files).

Safety/bugfixes retained from 0.5.3:
- Proper 'pipe' separator, safe html_escape, fixed ID_NAME_HINTS and ISO DATETIME regex, robust readers.
"""

import argparse
import csv
import re
from datetime import datetime
from pathlib import Path
from typing import Optional, Tuple, Dict, List, Set

import numpy as np
import pandas as pd

# =======================
# File picker (Tkinter optional)
# =======================
try:
    import tkinter as tk
    from tkinter import filedialog
    TK_AVAILABLE = True
except Exception:
    TK_AVAILABLE = False

DEFAULT_DIR = Path.cwd()

def pick_file_dialog() -> Optional[Path]:
    if not TK_AVAILABLE:
        return None
    root = tk.Tk(); root.withdraw()
    selected = filedialog.askopenfilename(
        initialdir=str(DEFAULT_DIR),
        title="Select the data file",
        filetypes=[("CSV/TSV", "*.csv *.tsv *.txt"), ("Excel", "*.xlsx *.xls"), ("All", "*.*")],
    )
    root.destroy()
    return Path(selected) if selected else None

# =======================
# IO helpers
# =======================
SEP_ALIASES = {"tab": "\t", "comma": ",", "semicolon": ";", "pipe": "|"}

def normalize_sep_arg(sep_arg: Optional[str]) -> Optional[str]:
    if not sep_arg:
        return None
    s = sep_arg.strip().lower()
    return SEP_ALIASES.get(s, sep_arg)

def detect_bom_encoding(raw: bytes) -> Optional[str]:
    if raw.startswith(b"\xff\xfe\x00\x00"): return "utf-32le"
    if raw.startswith(b"\x00\x00\xfe\xff"): return "utf-32be"
    if raw.startswith(b"\xff\xfe"): return "utf-16le"
    if raw.startswith(b"\xfe\xff"): return "utf-16be"
    if raw.startswith(b"\xef\xbb\xbf"): return "utf-8-sig"
    return None

def try_chardet(raw: bytes) -> Optional[str]:
    try:
        import chardet  # type: ignore
        guess = chardet.detect(raw)
        return guess.get("encoding")
    except Exception:
        return None

def detect_separator(text: str, max_lines: int = 50) -> Optional[str]:
    lines = [ln for ln in text.splitlines() if ln.strip()][:max_lines]
    if not lines:
        return None
    sample = "\n".join(lines)
    try:
        dialect = csv.Sniffer().sniff(sample, delimiters=[",", ";", "\t", "|"])
        return dialect.delimiter
    except Exception:
        pass
    candidates = [",", ";", "\t", "|"]
    best_sep, best_score = None, -1e9
    for sep in candidates:
        counts = [ln.count(sep) for ln in lines]
        mean_c = float(np.mean(counts)); var_c = float(np.var(counts))
        score = mean_c - var_c
        if mean_c >= 1 and score > best_score:
            best_sep, best_score = sep, score
    return best_sep

def smart_read_csv_with_guess(path: Path, forced_encoding: Optional[str] = None, forced_sep: Optional[str] = None) -> Tuple[pd.DataFrame, dict]:
    if forced_encoding or forced_sep:
        enc = forced_encoding or "utf-8"
        sep = normalize_sep_arg(forced_sep) or None
        if sep is None:
            with open(path, "rb") as f:
                raw = f.read(200_000)
            text = raw.decode(enc, errors="replace"); sep = detect_separator(text) or ","
        df = pd.read_csv(path, sep=sep, encoding=enc, engine="python")
        return df, {"encoding": enc, "sep": sep, "forced": True}

    with open(path, "rb") as f:
        raw = f.read(200_000)
    encodings_to_try: List[str] = []
    bom = detect_bom_encoding(raw)
    if bom: encodings_to_try.append(bom)
    encodings_to_try += ["utf-8", "utf-8-sig", "latin1", "cp1252", "utf-16", "utf-16le", "utf-16be", "utf-32"]
    ch = try_chardet(raw)
    if ch and ch not in encodings_to_try: encodings_to_try.append(ch)

    last_err = None
    for enc in encodings_to_try:
        try:
            text = raw.decode(enc, errors="replace")
        except Exception as e:
            last_err = e; continue
        sep = detect_separator(text) or None
        try:
            if sep:
                df = pd.read_csv(path, sep=sep, encoding=enc, engine="python"); return df, {"encoding": enc, "sep": sep, "forced": False}
            else:
                df = pd.read_csv(path, sep=None, encoding=enc, engine="python"); return df, {"encoding": enc, "sep": "auto", "forced": False}
        except Exception as e:
            last_err = e
        # brute-force separators
        for sep_try in [",", ";", "\t", "|"]:
            try:
                df = pd.read_csv(path, sep=sep_try, encoding=enc, engine="python"); return df, {"encoding": enc, "sep": sep_try, "forced": False}
            except Exception as e2:
                last_err = e2
    raise ValueError(f"Could not read CSV. Last error: {last_err}")

def select_excel_sheet_with_most_data(xlsx_path: Path, max_rows_sample: int = 5000) -> str:
    xl = pd.ExcelFile(xlsx_path)
    best, score = None, -1
    for sh in xl.sheet_names:
        try:
            df_tmp = xl.parse(sh, nrows=max_rows_sample)
            sc = int(df_tmp.notna().sum().sum())
            if sc > score:
                score, best = sc, sh
        except Exception:
            pass
    return best or xl.sheet_names[0]

def smart_read_any(path: Path, forced_encoding: Optional[str] = None, forced_sep: Optional[str] = None, input_sheet: Optional[str] = None) -> Tuple[pd.DataFrame, dict]:
    if path.suffix.lower() in {'.xlsx', '.xls'}:
        sheet = input_sheet or select_excel_sheet_with_most_data(path)
        df = pd.read_excel(path, sheet_name=sheet)
        return df, {"encoding": "binary", "sep": f"excel:{sheet}", "sheet": sheet, "forced": False}
    return smart_read_csv_with_guess(path, forced_encoding, forced_sep)

# =======================
# Inference & checks
# =======================
EMAIL_RE = re.compile(r'^[A-Za-z0-9._%+\-]+@[A-Za-z0-9.\-]+\.[A-Za-z]{2,}$')
ALNUM_RE = re.compile(r'^[A-Za-z0-9\-_]+$')
ISO_DATE_RE = re.compile(r"^\d{4}-\d{2}-\d{2}$")
ISO_DATETIME_RE = re.compile(r"^\d{4}-\d{2}-\d{2}(?:[ T]\d{2}:\d{2}:\d{2})$")

def parse_cfg_date(val):
    if val in (None, ""):
        return None
    s = str(val).strip()
    if ISO_DATE_RE.match(s):
        try:
            return pd.to_datetime(s, format="%Y-%m-%d", errors="coerce")
        except Exception:
            return pd.to_datetime(s, errors="coerce", dayfirst=False)
    if ISO_DATETIME_RE.match(s):
        try:
            return pd.to_datetime(s, format="%Y-%m-%d %H:%M:%S", errors="coerce")
        except Exception:
            return pd.to_datetime(s, errors="coerce", dayfirst=False)
    return pd.to_datetime(s, errors="coerce", dayfirst=True)

NBSP = '\xa0'
CURRENCY_SIGNS = set(list('$€£¥'))

def clean_numeric_string(s: str) -> str:
    s = s.replace(NBSP, ' ').strip()
    s = s.replace(' ', '')
    if s and s[0] in CURRENCY_SIGNS:
        s = s[1:]
    if s and s[-1] in CURRENCY_SIGNS:
        s = s[:-1]
    return s

def parse_num_locale(val) -> Optional[float]:
    if val in (None, "", np.nan):
        return None
    if isinstance(val, (int, float)) and not pd.isna(val):
        return float(val)
    s = clean_numeric_string(str(val))
    # dot thousands + comma decimal: 1.234,56 -> 1234.56
    if re.match(r"^\d{1,3}(\.\d{3})+,\d+$", s):
        s = s.replace('.', '').replace(',', '.')
    # comma thousands + dot decimal: 1,234.56 -> 1234.56
    elif re.match(r"^\d{1,3}(,\d{3})+\.\d+$", s):
        s = s.replace(',', '')
    # only comma decimal: 123,45 -> 123.45
    elif re.match(r"^\d+,\d+$", s):
        s = s.replace(',', '.')
    # only thousands (dot or comma)
    elif re.match(r"^\d{1,3}[\.,]\d{3}$", s):
        s = s.replace(',', '').replace('.', '')
    try:
        return float(s)
    except Exception:
        return None

def infer_kind(s: pd.Series, sample_size: int = 500) -> str:
    s_non = s.dropna()
    if len(s_non) == 0:
        return "free_text"
    sample = s_non.sample(min(sample_size, len(s_non)), random_state=42).astype(str).str.strip()
    if sample.str.fullmatch(EMAIL_RE).mean() > 0.6:
        return "email"
    if sample.str.startswith(("http://", "https://")).mean() > 0.6:
        return "url"
    iso_mask = sample.str.match(ISO_DATE_RE) | sample.str.match(ISO_DATETIME_RE)
    if iso_mask.mean() > 0.7:
        if pd.to_datetime(sample[iso_mask], errors="coerce", dayfirst=False).notna().mean() > 0.7:
            return "date"
    else:
        if pd.to_datetime(sample, errors="coerce", dayfirst=True).notna().mean() > 0.7:
            return "date"
    parsed = sample.apply(parse_num_locale)
    if parsed.notna().mean() > 0.7:
        inside_0_1 = ((parsed>=0)&(parsed<=1)).mean()
        inside_0_100=((parsed>=0)&(parsed<=100)).mean()
        if sample.str.contains('%', regex=False).mean() > 0.05 or max(inside_0_1, inside_0_100) > 0.85:
            return "percent"
        return "numeric"
    nunique = s_non.nunique(dropna=True)
    uniq_ratio = nunique / len(s_non)
    avg_len = sample.str.len().mean()
    if uniq_ratio > 0.9 and avg_len <= 40 and sample.str.fullmatch(ALNUM_RE).mean() > 0.7:
        return "id"
    vc = s_non.astype(str).value_counts(dropna=True)
    top10_cov = (vc.head(10).sum() / len(s_non)) if len(vc) else 0.0
    if (len(vc) <= 10 and top10_cov >= 0.8):
        return "categorical"
    return "free_text"

# =======================
# Criticality mapping
# =======================
CRIT_TO_NUM = {"LOW": 1, "MEDIUM": 2, "HIGH": 3, "VERY_HIGH": 4}
NUM_TO_CRIT = {v: k for k, v in CRIT_TO_NUM.items()}
ISSUE_DEFAULT = {
    # Default (used in INITIAL or when no column-level priority is set)
    "missing": "HIGH",  # note: in FINAL the column priority will override this
    "numeric_parse_fail": "HIGH",
    "numeric_out_of_range": "HIGH",
    "outlier_iqr": "MEDIUM",
    "outlier_magnitude": "HIGH",
    "percent_parse_fail": "HIGH",
    "percent_out_of_range": "HIGH",
    "percent_outlier_iqr": "MEDIUM",
    "date_parse_fail": "HIGH",
    "date_out_of_bounds": "VERY_HIGH",
    "email_format": "HIGH",
    "url_format": "MEDIUM",
    "domain_violation": "HIGH",
    "regex_violation": "HIGH",
    "text_length_outlier": "LOW",
    "weird_symbols_ratio": "MEDIUM",
}

def crit_num(issue_code: str) -> int:
    return CRIT_TO_NUM[ISSUE_DEFAULT.get(issue_code, "LOW")]

# =======================
# Helpers
# =======================

def parse_bool(x, default=False) -> bool:
    if x is None or (isinstance(x, float) and pd.isna(x)):
        return default
    if isinstance(x, bool):
        return x
    s = str(x).strip().lower()
    return s in {"true", "1", "yes", "y", "si", "sí", "on"}

def to_priority_num(s) -> int:
    if not isinstance(s, str):
        return 0
    return CRIT_TO_NUM.get(s.strip().upper(), 0)

# =======================
# Severity builder (FINAL override of column priority) + DEBUG
# =======================

def iqr_outliers(x: pd.Series, k: float = 1.5) -> pd.Series:
    x = pd.to_numeric(x, errors="coerce")
    q1 = x.quantile(0.25); q3 = x.quantile(0.75); iqr = q3 - q1
    if pd.isna(iqr) or iqr == 0:
        return pd.Series(False, index=x.index)
    return (x < q1 - k*iqr) | (x > q3 + k*iqr)

def magnitude_outliers(x: pd.Series, factor: float = 1e3) -> pd.Series:
    x = pd.to_numeric(x, errors="coerce"); med = x.median()
    if pd.isna(med) or med == 0:
        return pd.Series(False, index=x.index)
    return x > abs(med) * factor

CONFIG_COLUMNS = [
    'column', 'type', 'min', 'max', 'domain', 'regex', 'priority', 'active', 'highlight_nulls',
    'include_null_in_email', 'email_include_issues'
]

ES_TO_EN_COLS = {
    'columna': 'column',
    'tipo': 'type',
    'min_propuesto': 'min',
    'max_propuesto': 'max',
    'dominio': 'domain',
    'regex': 'regex',
    'prioridad': 'priority',
    'activo': 'active',
    'resaltar_null': 'highlight_nulls',
}

def normalize_config_headers(cfg: pd.DataFrame) -> pd.DataFrame:
    cols = [str(c).strip().lower() for c in cfg.columns]
    cols_mapped = [ES_TO_EN_COLS.get(c, c) for c in cols]
    cfg.columns = cols_mapped
    for c in CONFIG_COLUMNS:
        if c not in cfg.columns:
            if c == 'priority':
                cfg[c] = 'MEDIUM'
            elif c in ('active','highlight_nulls','include_null_in_email','email_include_issues'):
                cfg[c] = 'NO' if c in ('include_null_in_email','email_include_issues') else 'YES'
            else:
                cfg[c] = ''
    return cfg[CONFIG_COLUMNS].copy()


def build_severity_simple(
    df: pd.DataFrame,
    config_rows: pd.DataFrame,
    coverage_target: float,
    future_days: int,
    main_id_col: Optional[str],
    debug: bool = False,
    rules_dump: Optional[List[dict]] = None,
    apply_only_config: bool = False,
) -> Tuple[pd.DataFrame, Dict[str, dict], dict]:
    """
    apply_only_config=False  -> INITIAL mode: apply defaults (outliers, format checks, etc.)
    apply_only_config=True   -> FINAL  mode: only min/max, domain, regex from CONFIG; column PRIORITY overrides severities.
    """
    raw = config_rows.copy(); raw.columns = [str(c).strip().lower() for c in raw.columns]

    # Find Emails row (control)
    emails_row = None
    if 'column' in raw.columns:
        mask = raw['column'].astype(str).str.strip().str.lower().eq('emails')
        if mask.any(): emails_row = raw[mask].iloc[-1]
    elif 'columna' in raw.columns:
        mask = raw['columna'].astype(str).str.strip().str.lower().eq('emails')
        if mask.any(): emails_row = raw[mask].iloc[-1]

    cfg = normalize_config_headers(config_rows.copy())

    control: Dict[str, str] = {}
    forced_main_id: Optional[str] = None

    mask_emails = cfg['column'].astype(str).str.strip().str.lower().eq('emails')
    if mask_emails.any():
        last = cfg[mask_emails].iloc[-1]
        control = {
            'email_to': str(last.get('min','') or '').strip(),
            'group_col': str(last.get('domain','') or '').strip(),
            'generate': parse_bool(last.get('active','NO'), default=False),
            'email_severity_threshold': 'HIGH',
            'email_summary': True,
            'email_recipients_column': '',
        }
        if emails_row is not None:
            est = str(emails_row.get('email_severity_threshold','') or '').strip().upper()
            if est in ('HIGH','VERY_HIGH'): control['email_severity_threshold'] = est
            control['email_summary'] = parse_bool(emails_row.get('email_summary',''), default=True)
            erc = str(emails_row.get('email_recipients_column','') or '').strip()
            if erc: control['email_recipients_column'] = erc
        cfg = cfg[~mask_emails]

    # Main ID (meta)
    mask_mainid = cfg['column'].astype(str).str.strip().str.lower().eq('main id')
    if mask_mainid.any():
        row = cfg[mask_mainid].iloc[-1]
        forced_main_id = str(row.get('domain','') or '').strip()
        cfg = cfg[~mask_mainid]

    # Build rules map
    rule_map: Dict[str, dict] = {}
    for _, r in cfg.iterrows():
        col = str(r.get('column','') or '').strip()
        if not col:
            continue
        rule_map[col] = {
            'type': str(r.get('type','') or '').strip().lower(),
            'min': r.get('min', None),
            'max': r.get('max', None),
            'domain': r.get('domain', None),
            'regex': r.get('regex', None),
            'priority': to_priority_num(r.get('priority','MEDIUM')),
            'active': parse_bool(r.get('active','YES'), default=True),
            'highlight_nulls': parse_bool(r.get('highlight_nulls','YES'), default=True),
            'include_null_in_email': parse_bool(r.get('include_null_in_email','NO'), default=False),
            'email_include_issues': parse_bool(r.get('email_include_issues','NO'), default=False),
        }

    sev = pd.DataFrame(0, index=df.index, columns=df.columns, dtype=np.int8)

    for col in df.columns:
        s = df[col]
        rule = rule_map.get(col)
        ctype = infer_kind(s) if not rule or not rule.get('type') else rule['type']
        base_pri = 0 if not rule else int(rule['priority'])

        # helper to apply severity, respecting column priority override in FINAL
        def apply(col_sev_arr: np.ndarray, mask: pd.Series, issue_code: str) -> np.ndarray:
            default_num = crit_num(issue_code)
            num = base_pri if (apply_only_config and base_pri>0) else default_num
            if num <= 0:
                return col_sev_arr
            arr_mask = np.where(mask.to_numpy(), num, 0)
            return np.maximum(col_sev_arr, arr_mask)

        # start
        col_sev = np.zeros(len(s), dtype=np.int8)

        active = True if not rule else bool(rule['active'])
        dom_vals: Set[str] = set()
        if rule and rule.get('domain') and isinstance(rule.get('domain'), str):
            dom_vals = {v.strip() for v in str(rule.get('domain')).split(',') if v.strip()}

        regex_pat = None
        if rule and rule.get('regex') and isinstance(rule.get('regex'), str) and rule.get('regex').strip():
            try:
                regex_pat = re.compile(str(rule.get('regex')).strip())
            except re.error:
                regex_pat = None

        min_v = rule.get('min') if rule else None
        max_v = rule.get('max') if rule else None

        applied = {'column': col, 'type': ctype, 'priority': NUM_TO_CRIT.get(base_pri,''), 'min': None, 'max': None}

        # MISSING: always considered, but severity follows column priority in FINAL
        is_null = s.isna()
        if is_null.any():
            col_sev = apply(col_sev, is_null, 'missing')

        if not active:
            sev[col] = col_sev
            if rules_dump is not None:
                rules_dump.append({'column': col, 'type': ctype, 'priority': NUM_TO_CRIT.get(base_pri,''), 'min': None, 'max': None,
                                   'count_LOW': int((col_sev==1).sum()), 'count_MED': int((col_sev==2).sum()),
                                   'count_HIGH': int((col_sev==3).sum()), 'count_VHIGH': int((col_sev==4).sum())})
            continue

        try:
            if apply_only_config:
                # FINAL: only explicit rules; no automatic defaults/outliers
                if ctype == 'numeric' and (min_v not in (None, '') or max_v not in (None, '')):
                    x = pd.to_numeric(s.apply(lambda v: clean_numeric_string(str(v)) if pd.notna(v) else v), errors='coerce')
                    parse_fail = s.notna() & x.isna()
                    if parse_fail.any():
                        col_sev = apply(col_sev, parse_fail, 'numeric_parse_fail')
                    lo = parse_num_locale(min_v); hi = parse_num_locale(max_v)
                    applied['min'] = lo; applied['max'] = hi
                    if lo is not None:
                        col_sev = apply(col_sev, x < lo, 'numeric_out_of_range')
                    if hi is not None:
                        col_sev = apply(col_sev, x > hi, 'numeric_out_of_range')

                elif ctype == 'percent' and (min_v not in (None, '') or max_v not in (None, '')):
                    s_stripped = s.astype(str).str.replace('%','', regex=False)
                    x = pd.to_numeric(s_stripped.apply(clean_numeric_string), errors='coerce')
                    parse_fail = s.notna() & x.isna()
                    if parse_fail.any():
                        col_sev = apply(col_sev, parse_fail, 'percent_parse_fail')
                    lo = parse_num_locale(min_v); hi = parse_num_locale(max_v)
                    applied['min'] = lo; applied['max'] = hi
                    if lo is not None:
                        col_sev = apply(col_sev, x < lo, 'percent_out_of_range')
                    if hi is not None:
                        col_sev = apply(col_sev, x > hi, 'percent_out_of_range')

                elif ctype == 'date' and (min_v not in (None, '') or max_v not in (None, '')):
                    x = pd.to_datetime(s, errors='coerce', dayfirst=True)
                    parse_fail = s.notna() & x.isna()
                    if parse_fail.any():
                        col_sev = apply(col_sev, parse_fail, 'date_parse_fail')
                    lower = parse_cfg_date(min_v) if min_v not in (None, '') else None
                    upper = parse_cfg_date(max_v) if max_v not in (None, '') else None
                    applied['min'] = str(lower) if lower is not None else None
                    applied['max'] = str(upper) if upper is not None else None
                    bad = pd.Series(False, index=s.index)
                    if lower is not None:
                        bad = bad | (x < lower)
                    if upper is not None:
                        bad = bad | (x > upper)
                    bad = bad & x.notna()
                    if bad.any():
                        col_sev = apply(col_sev, bad, 'date_out_of_bounds')

                # domain / regex
                if dom_vals:
                    bad = s.notna() & ~s.astype(str).isin(dom_vals)
                    if bad.any():
                        col_sev = apply(col_sev, bad, 'domain_violation')
                if regex_pat:
                    bad = s.notna() & ~s.astype(str).apply(lambda v: bool(regex_pat.fullmatch(str(v))))
                    if bad.any():
                        col_sev = apply(col_sev, bad, 'regex_violation')

            else:
                # INITIAL: previous behaviour (defaults + outliers)
                if ctype == 'numeric':
                    x = pd.to_numeric(s.apply(lambda v: clean_numeric_string(str(v)) if pd.notna(v) else v), errors='coerce')
                    parse_fail = s.notna() & x.isna()
                    if parse_fail.any():
                        col_sev = apply(col_sev, parse_fail, 'numeric_parse_fail')
                    lo = parse_num_locale(min_v); hi = parse_num_locale(max_v)
                    applied['min'] = lo; applied['max'] = hi
                    if lo is not None:
                        col_sev = apply(col_sev, x < lo, 'numeric_out_of_range')
                    if hi is not None:
                        col_sev = apply(col_sev, x > hi, 'numeric_out_of_range')
                    m_iqr = iqr_outliers(x, k=1.5)
                    if m_iqr.any():
                        col_sev = apply(col_sev, m_iqr, 'outlier_iqr')
                    m_mag = magnitude_outliers(x, factor=1e3)
                    if m_mag.any():
                        col_sev = apply(col_sev, m_mag, 'outlier_magnitude')

                elif ctype == 'percent':
                    s_stripped = s.astype(str).str.replace('%','', regex=False)
                    x = pd.to_numeric(s_stripped.apply(clean_numeric_string), errors='coerce')
                    parse_fail = s.notna() & x.isna()
                    if parse_fail.any():
                        col_sev = apply(col_sev, parse_fail, 'percent_parse_fail')
                    lo = parse_num_locale(min_v); hi = parse_num_locale(max_v)
                    applied['min'] = lo; applied['max'] = hi
                    lo_eff = 0.0 if lo is None else lo
                    hi_eff = 100.0 if hi is None else hi
                    bad = (x < lo_eff) | (x > hi_eff)
                    if bad.any():
                        col_sev = apply(col_sev, bad, 'percent_out_of_range')
                    m_iqr = iqr_outliers(x, k=1.5)
                    if m_iqr.any():
                        col_sev = apply(col_sev, m_iqr, 'percent_outlier_iqr')

                elif ctype == 'date':
                    x = pd.to_datetime(s, errors='coerce', dayfirst=True)
                    parse_fail = s.notna() & x.isna()
                    if parse_fail.any():
                        col_sev = apply(col_sev, parse_fail, 'date_parse_fail')
                    if x.notna().any():
                        lower = parse_cfg_date(min_v) if min_v not in (None, '') else pd.Timestamp('1900-01-01')
                        upper = parse_cfg_date(max_v) if max_v not in (None, '') else (pd.Timestamp.today()+pd.Timedelta(days=future_days))
                        applied['min'] = str(lower) if lower is not None else None
                        applied['max'] = str(upper) if upper is not None else None
                        bad = ((x < lower) | (x > upper)) & x.notna()
                        if bad.any():
                            col_sev = apply(col_sev, bad, 'date_out_of_bounds')

                elif ctype == 'email':
                    bad = s.notna() & ~s.astype(str).str.fullmatch(EMAIL_RE)
                    if bad.any():
                        col_sev = apply(col_sev, bad, 'email_format')

                elif ctype == 'url':
                    bad = s.notna() & ~s.astype(str).str.startswith(("http://", "https://"))
                    if bad.any():
                        col_sev = apply(col_sev, bad, 'url_format')

                elif ctype == 'categorical':
                    if dom_vals:
                        bad = s.notna() & ~s.astype(str).isin(dom_vals)
                        if bad.any():
                            col_sev = apply(col_sev, bad, 'domain_violation')
                    if regex_pat:
                        bad = s.notna() & ~s.astype(str).apply(lambda v: bool(regex_pat.fullmatch(str(v))))
                        if bad.any():
                            col_sev = apply(col_sev, bad, 'regex_violation')

                elif ctype == 'free_text':
                    s_str = s.dropna().astype(str)
                    if len(s_str) > 0:
                        lens = s.astype(str).str.len(); m_len = iqr_outliers(lens, k=2.0)
                        if m_len.any():
                            col_sev = apply(col_sev, m_len, 'text_length_outlier')
                        sym_ratio = s.astype(str).apply(lambda x: sum(ch in "!@#$%^&*()=+[]{}<>~`" for ch in x)/max(1,len(x)))
                        arr = sym_ratio.to_numpy(dtype=float); med=float(np.median(arr)); mad=float(np.median(np.abs(arr-med)))
                        thr = med + 5*mad if mad>0 else 0.5
                        m_sym = sym_ratio > thr
                        if m_sym.any():
                            col_sev = apply(col_sev, m_sym, 'weird_symbols_ratio')

        except Exception:
            pass

        sev[col] = col_sev.astype(np.int8)

        if rules_dump is not None:
            arr = col_sev
            applied['count_LOW']   = int((arr==1).sum())
            applied['count_MED']   = int((arr==2).sum())
            applied['count_HIGH']  = int((arr==3).sum())
            applied['count_VHIGH'] = int((arr==4).sum())
            rules_dump.append(applied)

    if forced_main_id:
        control['main_id_forced'] = forced_main_id

    return sev, rule_map, control

# =======================
# ID detection
# =======================
ID_NAME_HINTS = [r"\b(id|code|codigo|código|employee|empleado|vin|matricula|matrícula|plate|nif|dni|cif|cedula|cédula|passport)\b"]

def score_id_candidate(col_name: str, s: pd.Series) -> float:
    s_non = s.dropna(); total = len(s)
    if total == 0: return 0.0
    nunique = s_non.nunique(dropna=True)
    uniq_ratio = (nunique / len(s_non)) if len(s_non) else 0.0
    s_str = s_non.astype(str).str.strip()
    avg_len = float(s_str.str.len().mean()) if len(s_str) else 0.0
    alnum_ratio = float(s_str.str.fullmatch(ALNUM_RE).mean()) if len(s_str) else 0.0
    as_date = pd.to_datetime(s_non, errors='coerce', dayfirst=True).notna().mean() if len(s_non) else 0.0
    as_num  = pd.to_numeric(s_non, errors='coerce').notna().mean() if len(s_non) else 0.0
    name_hint = any(re.search(pat, col_name, flags=re.IGNORECASE) for pat in ID_NAME_HINTS)
    score=0.0
    if name_hint: score+=3.0
    if uniq_ratio>=0.98: score+=3.0
    elif uniq_ratio>=0.9: score+=2.0
    if 6<=avg_len<=40: score+=1.0
    if alnum_ratio>=0.8: score+=1.0
    if as_date<=0.05: score+=0.5
    if as_num<=0.5: score+=0.5
    return score

def detect_main_id(df: pd.DataFrame) -> str:
    if df.shape[1]==0: return 'row_id'
    best, best_score = df.columns[0], -1.0
    for c in df.columns:
        sc = score_id_candidate(c, df[c])
        if sc>best_score:
            best, best_score = c, sc
    return best

# =======================
# Export report
# =======================
from openpyxl.styles import PatternFill, Font

def export_excel(df_raw: pd.DataFrame, sev_df: pd.DataFrame, out_path: Path,
                 summary_extra: Dict[str, str], main_id_name: str,
                 rules_applied: Optional[pd.DataFrame] = None) -> None:
    out_dir = out_path.parent; out_dir.mkdir(parents=True, exist_ok=True)
    with pd.ExcelWriter(out_path, engine='openpyxl') as w:
        # Summary
        summary_rows={'metric':[], 'value':[]}
        for k,v in { 'rows': len(df_raw), 'columns': df_raw.shape[1], **summary_extra }.items():
            summary_rows['metric'].append(k); summary_rows['value'].append(v)
        pd.DataFrame(summary_rows).to_excel(w, index=False, sheet_name='Summary')

        # Dataset
        df_raw.to_excel(w, index=False, sheet_name='Dataset')

        # Main ID
        pd.DataFrame({'key':['main_id'], 'value':[main_id_name]}).to_excel(w, index=False, sheet_name='Main_ID')

        # Rules applied (optional)
        if rules_applied is not None:
            rules_applied.to_excel(w, index=False, sheet_name='Rules_Applied')

        # Coloring
        ws=w.sheets['Dataset']
        FILLS={
            1:PatternFill(fill_type='solid', fgColor='FFF2CC'),  # LOW
            2:PatternFill(fill_type='solid', fgColor='FCE4D6'),  # MED
            3:PatternFill(fill_type='solid', fgColor='F8CBAD'),  # HIGH
            4:PatternFill(fill_type='solid', fgColor='C00000'),  # VERY_HIGH (red)
        }
        WHITE=Font(color='FFFFFF')
        n_rows, n_cols = df_raw.shape
        for r in range(n_rows):
            for c in range(n_cols):
                sev=int(sev_df.iat[r,c])
                if sev>0:
                    cell = ws.cell(row=r+2, column=c+1)
                    cell.fill = FILLS.get(sev)
                    if sev==4:
                        cell.font=WHITE

# =======================
# Config generator + HELP + Validations
# =======================
from openpyxl.utils import get_column_letter
from openpyxl.worksheet.datavalidation import DataValidation


def _help_text() -> List[str]:
    return [
        "What is this file?",
        "This CONFIG workbook controls data QA checks and email drafting.",
        "",
        "How to use (quick):",
        "1) Row 'Emails': set active=YES and domain=<column to group by>.",
        "   - If the chosen column contains emails, one draft per address is generated.",
        "   - Otherwise, one draft per value is generated and recipients are taken from 'Contact' or any detected email column.",
        "   Optional: add extra recipients in the 'min' cell of 'Emails' row.",
        "   Optional: set 'email_recipients_column' to explicitly choose a recipients column.",
        "2) Row 'Main ID': set domain=<main ID column> to force the identifier used in the report and emails.",
        "3) Per-column area: set 'priority' (LOW/MEDIUM/HIGH/VERY_HIGH). In FINAL, all issues for that column will use this severity.",
        "   - include_null_in_email: NULLs in columns marked YES will appear in email drafts.",
        "   - email_include_issues: issues from columns marked YES will appear in email IssueColumns.",
        "4) INITIAL marks outliers by default (discovery). FINAL applies only your rules (min/max, domain, regex) and the column priority.",
        "",
        "Emails: only HIGH/VERY_HIGH issues per threshold; optional summary; never sends.",
    ]


def apply_config_validations(writer, cfg_df: pd.DataFrame, df_columns: List[str]) -> None:
    wb = writer.book; ws_cfg = writer.sheets['CONFIG']
    ws_lists = wb.create_sheet('LISTS'); ws_lists.sheet_state='hidden'
    types=["numeric","percent","date","email","url","categorical","free_text","id"]
    priorities=["LOW","MEDIUM","HIGH","VERY_HIGH"]
    yesno=["YES","NO"]
    for i,v in enumerate(types,1): ws_lists.cell(row=i, column=1, value=v)
    for i,v in enumerate(priorities,1): ws_lists.cell(row=i, column=2, value=v)
    for i,v in enumerate(yesno,1): ws_lists.cell(row=i, column=3, value=v)
    for i,v in enumerate(df_columns,1): ws_lists.cell(row=i, column=4, value=str(v))

    headers=[str(c).strip().lower() for c in cfg_df.columns]
    def col_idx(name): return headers.index(name)+1 if name in headers else None
    n_rows=len(cfg_df)+1

    dv_type=DataValidation(type="list", formula1="=LISTS!$A$1:$A$8", allow_blank=True)
    dv_pri=DataValidation(type="list", formula1="=LISTS!$B$1:$B$4", allow_blank=True)
    dv_yesno=DataValidation(type="list", formula1="=LISTS!$C$1:$C$2", allow_blank=True)
    dv_headers=DataValidation(type="list", formula1=f"=LISTS!$D$1:$D${len(df_columns)}", allow_blank=True)
    for dv in (dv_type,dv_pri,dv_yesno,dv_headers): ws_cfg.add_data_validation(dv)

    def apply_range(col_name, dv):
        c=col_idx(col_name);
        if not c: return
        dv.add(f"{get_column_letter(c)}2:{get_column_letter(c)}{n_rows}")

    apply_range('type', dv_type)
    apply_range('priority', dv_pri)
    apply_range('active', dv_yesno)
    apply_range('highlight_nulls', dv_yesno)
    apply_range('include_null_in_email', dv_yesno)
    apply_range('email_include_issues', dv_yesno)

    import pandas as _pd
    mask_emails=_pd.Series([str(x).strip().lower() for x in cfg_df['column']]).eq('emails')
    if mask_emails.any():
        r=cfg_df.index[mask_emails][0]+2
        c=col_idx('active'); 
        if c: dv_yesno.add(f"{get_column_letter(c)}{r}")
        c=col_idx('domain');
        if c: dv_headers.add(f"{get_column_letter(c)}{r}")
        if 'email_severity_threshold' in headers:
            dv_thr=DataValidation(type="list", formula1='"HIGH,VERY_HIGH"', allow_blank=True)
            ws_cfg.add_data_validation(dv_thr)
            c=col_idx('email_severity_threshold'); 
            if c: dv_thr.add(f"{get_column_letter(c)}{r}")
        if 'email_summary' in headers:
            c=col_idx('email_summary'); 
            if c: dv_yesno.add(f"{get_column_letter(c)}{r}")
        if 'email_recipients_column' in headers:
            c=col_idx('email_recipients_column')
            if c: dv_headers.add(f"{get_column_letter(c)}{r}")


def generate_simple_config(input_path: Path, df: pd.DataFrame, out_path: Path,
                           coverage_target: float, future_days: int, used_sheet: Optional[str]) -> None:
    rows=[]
    for col in df.columns:
        kind=infer_kind(df[col]); min_p=''; max_p=''; domain_vals=''; regex=''
        if kind in ('numeric','percent'):
            x = pd.to_numeric(df[col].astype(str).str.replace('%', '', regex=False), errors='coerce')
            if x.notna().any():
                try:
                    min_p=float(x.quantile(0.01)); max_p=float(x.quantile(0.99))
                except Exception:
                    min_p=''; max_p=''
        elif kind=='date':
            x = pd.to_datetime(df[col], errors='coerce', dayfirst=True)
            if x.notna().any():
                try:
                    min_p=x.quantile(0.01).date().isoformat(); max_p=x.quantile(0.99).date().isoformat()
                except Exception:
                    min_p=''; max_p=''
        elif kind=='categorical':
            vc = df[col].astype(str).value_counts(dropna=True)
            if len(vc)>0:
                domain_vals=",".join(map(str, vc.head(15).index.tolist()))
        rows.append({'column':col,'type':kind,'min':min_p,'max':max_p,'domain':domain_vals,'regex':regex,
                     'priority':'MEDIUM','active':'YES','highlight_nulls':'YES',
                     'include_null_in_email':'NO','email_include_issues':'NO'})
    rows.append({'column':'Emails','type':'meta','min':'','max':'','domain':'','regex':'','priority':'','active':'NO','highlight_nulls':'','include_null_in_email':'','email_include_issues':''})
    rows.append({'column':'Main ID','type':'meta','min':'','max':'','domain':detect_main_id(df),'regex':'','priority':'','active':'','highlight_nulls':'','include_null_in_email':'','email_include_issues':''})
    cfg=pd.DataFrame(rows, columns=CONFIG_COLUMNS)
    for c in ('email_severity_threshold','email_summary','email_recipients_column'):
        if c not in cfg.columns: cfg[c]=''
    idx = cfg.index[cfg['column'].astype(str).str.lower().eq('emails')]
    if len(idx)>0:
        i=idx[0]; cfg.at[i,'email_severity_threshold']='HIGH'; cfg.at[i,'email_summary']='YES'; cfg.at[i,'email_recipients_column']=''

    with pd.ExcelWriter(out_path, engine='openpyxl') as w:
        cfg.to_excel(w, index=False, sheet_name='CONFIG')
        meta_df=pd.DataFrame({'key':['input_file','input_sheet','coverage_target','future_days','generated_on','rows','cols'],
                              'value':[str(input_path.name),(used_sheet or ''),coverage_target,future_days,datetime.now().isoformat(),len(df),df.shape[1]]})
        meta_df.to_excel(w, index=False, sheet_name='META')
        ws_help = w.book.create_sheet('HELP')
        for r, line in enumerate(_help_text(), start=1): ws_help.cell(row=r, column=1, value=line)
        apply_config_validations(w, cfg_df=cfg, df_columns=list(df.columns))


def read_simple_config(cfg_path: Path) -> Tuple[pd.DataFrame, dict]:
    xl = pd.ExcelFile(cfg_path)
    cfg = xl.parse('CONFIG')
    return cfg.copy(), {'has_meta': 'META' in xl.sheet_names}

# =======================
# Email generation
# =======================

def extract_emails(text: str) -> List[str]:
    if not isinstance(text,str) or not text:
        return []
    pat=re.compile(r"[A-Za-z0-9._%+\-]+@[A-Za-z0-9.\-]+\.[A-Za-z]{2,}")
    return pat.findall(text)

import html as _html

def html_pill(label: str, bg="#eef2ff", fg="#1f2a6b") -> str:
    return f'<span style="display:inline-block;padding:2px 6px;border-radius:10px;background:{bg};color:{fg};font-size:12px;">{label}</span>'

def html_escape(s: str) -> str:
    return _html.escape(str(s), quote=True)

from typing import Iterable

def build_row_issue_summary_for_email(df: pd.DataFrame, sev_df: pd.DataFrame, id_col: str,
                                      include_null_cols: Set[str], threshold_num: int,
                                      include_issue_cols: Set[str]) -> pd.DataFrame:
    if id_col in df.columns:
        id_series = df[id_col].astype(str); id_name = id_col
    else:
        id_series = pd.Series([str(i) for i in df.index], index=df.index, name='row_id'); id_name='row_id'

    include_cols = [c for c in df.columns if c in include_null_cols]
    if include_cols:
        is_null_df = df[include_cols].isna()
        missing_cols_list = [", ".join(is_null_df.columns[is_null_df.loc[i]].tolist()) for i in df.index]
    else:
        missing_cols_list = ["" for _ in df.index]

    sev_num = sev_df.to_numpy(); cols = list(sev_df.columns); thr=max(3,threshold_num)

    allowed_issue_set = set(include_issue_cols)
    allowed_indices = [j for j, name in enumerate(cols) if name in allowed_issue_set]

    if allowed_indices:
        count_high = (sev_num[:, allowed_indices]==3).sum(axis=1)
        count_vhigh = (sev_num[:, allowed_indices]==4).sum(axis=1)
    else:
        count_high = np.zeros(sev_num.shape[0], dtype=int)
        count_vhigh = np.zeros(sev_num.shape[0], dtype=int)

    hv_cols=[]
    for i in range(sev_num.shape[0]):
        if allowed_indices:
            idx = [j for j in allowed_indices if sev_num[i, j] >= thr]
        else:
            idx = []
        names = [cols[j] for j in idx]
        hv_cols.append(", ".join(names[:6]) + (f" (+{len(names)-6} more)" if len(names)>6 else ""))

    return pd.DataFrame({ id_name: id_series.values, 'MissingFields': missing_cols_list, 'IssueColumns': hv_cols,
                          'Count_HIGH': count_high, 'Count_VHIGH': count_vhigh }, index=df.index)


def _auto_pick_group_col(df: pd.DataFrame) -> Optional[str]:
    if 'Contact' in df.columns: return 'Contact'
    for c in df.columns:
        ser = df[c].astype(str).fillna('')
        if ser.apply(lambda x: len(extract_emails(x))>0).mean() >= 0.2: return c
    for c in df.columns:
        nunique = df[c].nunique(dropna=True)
        if 1 < nunique <= 50: return c
    return None


def generate_emails(df: pd.DataFrame, sev_df: pd.DataFrame, control: dict, main_id: str,
                    out_dir: Path, default_contact_col: str = 'Contact', cfg_rules: Dict[str, dict] = None) -> None:
    if not control or not control.get('generate', False):
        return

    group_col_name = (control.get('group_col','') or '').strip()
    if not group_col_name:
        fallback = _auto_pick_group_col(df)
        if fallback:
            print(f"[Emails] 'domain' was empty. Auto-picking group column: '{fallback}'."); group_col_name = fallback
        else:
            print("[Emails] 'domain' is empty and no suitable column found. Skipping email generation."); return

    col_map = {str(c).strip(): c for c in df.columns}
    col_lower_map = {str(c).strip().lower(): c for c in df.columns}
    resolved_group_col = col_map.get(group_col_name) or col_lower_map.get(group_col_name.lower())
    if not resolved_group_col:
        fallback = _auto_pick_group_col(df)
        if fallback:
            print(f"[Emails] Group-by column '{group_col_name}' not found. Auto-picking '{fallback}'."); resolved_group_col = fallback
        else:
            print(f"[Emails] Group-by column '{group_col_name}' not found. Skipping."); return

    thr_label = (control.get('email_severity_threshold','HIGH') or 'HIGH').strip().upper(); thr_num = CRIT_TO_NUM.get(thr_label,3)

    include_null_cols: Set[str] = set()
    include_issue_cols: Set[str] = set()
    if cfg_rules:
        for col,rule in cfg_rules.items():
            try:
                if rule.get('include_null_in_email', False):
                    include_null_cols.add(col)
                if rule.get('email_include_issues', False):
                    include_issue_cols.add(col)
            except Exception:
                pass

    recipients_pref = (control.get('email_recipients_column','') or '').strip()

    series_grp = df[resolved_group_col].astype(str).fillna('')
    email_like_ratio = series_grp.apply(lambda x: len(extract_emails(x))>0).mean() if len(series_grp) else 0.0
    if email_like_ratio >= 0.2:
        email_to_indices: Dict[str, List[int]] = {}
        for i, cell in df[resolved_group_col].fillna('').astype(str).items():
            for em in (e.lower() for e in extract_emails(cell)):
                email_to_indices.setdefault(em, []).append(i)
        grouping = email_to_indices; contact_col = resolved_group_col
    else:
        groups = df[resolved_group_col].fillna('(no value)')
        grouping = {g: df.index[groups==g] for g in groups.unique()}
        contact_col = None

    if recipients_pref and recipients_pref in df.columns:
        contact_col = recipients_pref
    elif default_contact_col in df.columns:
        contact_col = default_contact_col
    else:
        for c in df.columns:
            ser = df[c].astype(str).fillna('')
            if ser.apply(lambda x: len(extract_emails(x))>0).mean() >= 0.1:
                contact_col = c; print(f"[Emails] Using column '{contact_col}' for recipients."); break

    if not contact_col:
        print("[Emails] No column with emails found for recipients. Drafts will be saved without 'To:'.")

    outlook=None
    try:
        import win32com.client  # type: ignore
        outlook = win32com.client.Dispatch("Outlook.Application")
    except Exception:
        outlook=None; print("[Emails] Outlook/pywin32 not available. Will save .html instead of .msg.")

    out_dir.mkdir(parents=True, exist_ok=True)
    extra_to = (control.get('email_to','') or '').strip(); generated=0

    row_summary_all = build_row_issue_summary_for_email(
        df, sev_df, id_col=main_id,
        include_null_cols=include_null_cols, threshold_num=thr_num,
        include_issue_cols=include_issue_cols
    )

    for key, idxs in grouping.items():
        if len(idxs)==0: continue
        sub = df.loc[idxs]; sub_sum = row_summary_all.loc[idxs]
        has_hv = (sub_sum[['Count_HIGH','Count_VHIGH']].sum(axis=1) > 0)
        has_miss = sub_sum['MissingFields'].astype(str).str.len() > 0
        show = sub_sum[has_hv | has_miss]
        if show.empty: continue

        to_list: List[str] = []
        if extra_to:
            to_list.extend(extract_emails(extra_to))
        if contact_col and contact_col in sub.columns:
            for val in sub[contact_col].dropna().astype(str):
                to_list.extend(extract_emails(val))
        if email_like_ratio >= 0.2:
            if isinstance(key,str) and key:
                to_list.append(key)
        to_list = sorted({e for e in to_list if e})

        # build HTML rows
        rows_html=[]; id_header = html_escape(main_id) if main_id in df.columns else 'Row ID'
        # omit MissingFields column if no row has missing
        include_missing_col = show['MissingFields'].astype(str).str.len().gt(0).any()

        for _, r in show.iterrows():
            id_val = html_escape(r[main_id]) if main_id in show.columns else html_escape(r.name)
            miss = html_escape(r['MissingFields']) if (include_missing_col and r['MissingFields']) else ''
            issues = html_escape(r['IssueColumns']) if r['IssueColumns'] else ''
            pills=[]
            if int(r['Count_VHIGH'])>0: pills.append(html_pill(f"VHIGH {int(r['Count_VHIGH'])}", "#ffe3e6", "#86181d"))
            if int(r['Count_HIGH'])>0: pills.append(html_pill(f"HIGH {int(r['Count_HIGH'])}", "#f8d7da", "#721c24"))
            if include_missing_col:
                rows_html.append(f"""
<tr style=\"border-bottom:1px solid #eaeaea;\">
  <td style=\"padding:6px 8px; white-space:nowrap;\">{id_val}</td>
  <td style=\"padding:6px 8px;\">{miss}</td>
  <td style=\"padding:6px 8px;\">{issues}</td>
  <td style=\"padding:6px 8px;\">{' '.join(pills) if pills else ''}</td>
</tr>
""")
            else:
                rows_html.append(f"""
<tr style=\"border-bottom:1px solid #eaeaea;\">
  <td style=\"padding:6px 8px; white-space:nowrap;\">{id_val}</td>
  <td style=\"padding:6px 8px;\">{issues}</td>
  <td style=\"padding:6px 8px;\">{' '.join(pills) if pills else ''}</td>
</tr>
""")
        label = key if isinstance(key,str) else str(key)
        subject = f"[Data QA] Items to review — Group: {label}"
        if include_missing_col:
            headers_html = f"""
      <th style=\"text-align:left;padding:6px 8px;\">{id_header}</th>
      <th style=\"text-align:left;padding:6px 8px;\">Missing Fields</th>
      <th style=\"text-align:left;padding:6px 8px;\">Issue Columns (HV)</th>
      <th style=\"text-align:left;padding:6px 8px;\">Severities</th>
            """
        else:
            headers_html = f"""
      <th style=\"text-align:left;padding:6px 8px;\">{id_header}</th>
      <th style=\"text-align:left;padding:6px 8px;\">Issue Columns (HV)</th>
      <th style=\"text-align:left;padding:6px 8px;\">Severities</th>
            """
        body_html = f"""
<html><body style=\"font-family:Segoe UI, Arial, sans-serif; font-size:13px;\">
<p>Hello,</p>
<p>Please find the items detected for <b>{html_escape(label)}</b>. This draft includes only HIGH/VERY_HIGH issues (from CONFIG rules) and user-selected missing fields.</p>
<table style=\"border-collapse:collapse; width:100%; margin-top:8px;\">
  <thead>
    <tr style=\"background:#f0f3f6; border-bottom:1px solid #d0d7de;\">
{headers_html}
    </tr>
  </thead>
  <tbody>{''.join(rows_html)}</tbody>
</table>
<p style=\"margin-top:12px;\">Thanks,<br/>Daniel</p>
</body></html>
"""
        safe = re.sub(r"[^A-Za-z0-9._\- ]+", "_", label)[:100]
        if outlook is not None:
            try:
                mail = outlook.CreateItem(0); mail.Subject = subject
                if to_list: mail.To = '; '.join(to_list)
                mail.HTMLBody = body_html
                msg_path = out_dir / f"QA_group_{safe}.msg"; mail.SaveAs(str(msg_path), 3)
                print(f"Saved MSG -> {msg_path.name} To={mail.To or '(none)'}"); generated += 1; continue
            except Exception as e:
                print(f"Could not save .msg, falling back to .html. Details: {e}")
        html_path = out_dir / f"QA_group_{safe}.html"; open(html_path,'w',encoding='utf-8').write(body_html)
        print(f"Saved HTML -> {html_path.name} Suggested To={', '.join(to_list) if to_list else '-'}"); generated += 1

    print(f"Email drafts generated: {generated}")

# =======================
# MAIN with --debug and FINAL strict mode
# =======================

def main():
    ap = argparse.ArgumentParser(description='Universal CSV/Excel Analyzer (EN V0.5.4) — simple config (no sending)')
    ap.add_argument('-i','--input', help='Path to CSV/TSV/Excel (if omitted, opens a file picker)')
    ap.add_argument('-o','--out', help='Output Excel path (default initial/final next to input)')
    ap.add_argument('--config', help='Path to config_[input].xlsx (if omitted, autodetects)')
    ap.add_argument('--encoding', help='Force encoding (utf-16, utf-8, latin1, ...)')
    ap.add_argument('--sep', help=r'Force separator: tab, comma, semicolon, pipe or the symbol (\t, ;, , , |)')
    ap.add_argument('--input-sheet', help='Excel sheet to use (override)')
    ap.add_argument('--coverage', type=float, default=0.95, help='(Reserved) Coverage target (not used)')
    ap.add_argument('--future-days', type=int, default=730, help='Allowed future window for dates (days) — INITIAL only')
    ap.add_argument('--id-col', default='', help='Main ID column (if known)')
    ap.add_argument('--debug', action='store_true', help='Export diagnostics and print parsed limits per column')

    args = ap.parse_args()

    if args.input:
        input_path = Path(args.input)
        if not input_path.exists():
            print(f"File does not exist: {input_path}"); return
    else:
        input_path = pick_file_dialog();
        if not input_path:
            print('Operation cancelled.'); return

    if not args.config:
        auto_cfg = input_path.with_name(f"config_{input_path.stem}.xlsx")
        if auto_cfg.exists():
            print(f"Config detected: {auto_cfg.name} — FINAL phase"); args.config = str(auto_cfg)

    df, meta = smart_read_any(input_path, forced_encoding=args.encoding, forced_sep=args.sep, input_sheet=args.input_sheet)
    used_sheet = meta.get('sheet') if isinstance(meta.get('sep',''),str) and str(meta.get('sep','')).startswith('excel:') else None
    print(f"Read: {input_path.name} sep={meta.get('sep')} shape={df.shape}")

    main_id = args.id_col.strip() or ''

    if args.config:
        # FINAL phase (strict + column priority override)
        cfg_path = Path(args.config); cfg_df, _ = read_simple_config(cfg_path)
        rules_dump: List[dict] = []
        sev_df, rule_map, control = build_severity_simple(
            df, cfg_df, coverage_target=args.coverage, future_days=args.future_days,
            main_id_col=main_id, debug=args.debug, rules_dump=rules_dump, apply_only_config=True)

        if args.debug:
            for rec in rules_dump:
                if rec.get('min') is not None or rec.get('max') is not None:
                    print(f"[DEBUG] {rec['column']} type={rec['type']} min={rec['min']} max={rec['max']} sev_counts(L/M/H/VH)={rec.get('count_LOW',0)}/{rec.get('count_MED',0)}/{rec.get('count_HIGH',0)}/{rec.get('count_VHIGH',0)}")
        rules_applied_df = pd.DataFrame(rules_dump) if rules_dump else None

        if not main_id:
            forced = control.get('main_id_forced',''); 
            if forced and forced in df.columns: main_id = forced
        if not main_id:
            main_id = detect_main_id(df)

        ts = datetime.now().strftime('%Y%m%d_%H%M')
        out_xlsx = Path(args.out) if args.out else input_path.with_name(f"final_report_{input_path.stem}_{ts}.xlsx")
        extra={'phase':'FINAL','input_sheet': used_sheet or '', 'main_id': main_id, 'config_file': Path(args.config).name}
        export_excel(df, sev_df, out_xlsx, extra, main_id, rules_applied=rules_applied_df)
        print(f"Final report -> {out_xlsx}")

        if args.debug:
            # Diagnostics for numeric/percent/date with limits
            diag_dir = out_xlsx.parent / f"QA_DIAG_{input_path.stem}_{ts}"
            diag_dir.mkdir(parents=True, exist_ok=True)
            for rec in rules_dump or []:
                if rec['type'] in ('numeric','percent','date') and (rec.get('min') is not None or rec.get('max') is not None):
                    col = rec['column']
                    try:
                        sampler = df[[col]].copy()
                        if rec['type']=='numeric':
                            sampler['parsed'] = pd.to_numeric(sampler[col].apply(lambda v: clean_numeric_string(str(v)) if pd.notna(v) else v), errors='coerce')
                            lo, hi = rec['min'], rec['max']
                            sampler['below_min'] = sampler['parsed'] < lo if lo is not None else False
                            sampler['above_max'] = sampler['parsed'] > hi if hi is not None else False
                        elif rec['type']=='percent':
                            sampler['parsed'] = pd.to_numeric(sampler[col].astype(str).str.replace('%','', regex=False).apply(clean_numeric_string), errors='coerce')
                            lo = rec['min']; hi = rec['max']
                            sampler['out_of_range'] = (sampler['parsed'] < lo) | (sampler['parsed'] > hi)
                        elif rec['type']=='date':
                            sampler['parsed'] = pd.to_datetime(sampler[col], errors='coerce', dayfirst=True)
                            from_dt = pd.to_datetime(rec['min']) if rec.get('min') else None
                            to_dt = pd.to_datetime(rec['max']) if rec.get('max') else None
                            out = pd.Series(False, index=sampler.index)
                            if from_dt is not None:
                                out = out | (sampler['parsed'] < from_dt)
                            if to_dt is not None:
                                out = out | (sampler['parsed'] > to_dt)
                            sampler['out_of_bounds'] = out
                        sampler.head(5000).to_excel(diag_dir / f"{col[:60]}_diag.xlsx", index=False)
                    except Exception:
                        pass
            print(f"[DEBUG] Diagnostics saved -> {diag_dir}")

        generate_emails(df, sev_df, control=control, main_id=main_id, out_dir=out_xlsx.parent, cfg_rules=rule_map)
        return

    # INITIAL phase
    if not main_id: main_id = detect_main_id(df)

    tmp_cfg = pd.DataFrame([
        {
            'column': c,
            'type': infer_kind(df[c]),
            'min':'', 'max':'', 'domain':'', 'regex':'',
            'priority':'MEDIUM','active':'YES','highlight_nulls':'YES',
            'include_null_in_email':'NO','email_include_issues':'NO'
        } for c in df.columns
    ])

    sev_df, _, _ = build_severity_simple(
        df, tmp_cfg, coverage_target=args.coverage, future_days=args.future_days,
        main_id_col=main_id, debug=args.debug, rules_dump=None, apply_only_config=False)

    ts = datetime.now().strftime('%Y%m%d_%H%M')
    out_xlsx = Path(args.out) if args.out else input_path.with_name(f"initial_report_{input_path.stem}_{ts}.xlsx")
    extra={'phase':'INITIAL','input_sheet': used_sheet or '', 'main_id': main_id}
    export_excel(df, sev_df, out_xlsx, extra, main_id)
    print(f"Initial report -> {out_xlsx}")

    cfg_out = input_path.with_name(f"config_{input_path.stem}.xlsx")
    generate_simple_config(input_path, df, cfg_out, coverage_target=args.coverage, future_days=args.future_days, used_sheet=used_sheet)
    print("Simple config -> {}".format(cfg_out))
    print("Edit 'CONFIG':\n - Row 'Emails': active=YES, domain=<group column>, email_recipients_column=<optional>\n - Row 'Main ID': domain=<main ID column>\n - Per column: set priority (LOW/MEDIUM/HIGH/VERY_HIGH). In FINAL, missing & issues adopt that severity.\n - Mark include_null_in_email=YES to include missing in emails; email_include_issues=YES for issue columns.\nRun again to produce FINAL + email drafts (.msg/.html).")

if __name__ == '__main__':
    main()
